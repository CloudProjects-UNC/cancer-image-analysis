Cloud Dataproc

Create a cluster:
gcloud dataproc clusters create [CLUSTER_NAME]

Get information on created cluster:
cloud dataproc clusters describe [CLUSTER_NAME]

#create firewall 
gcloud compute --project=rashnilsandeep firewall-rules create cancerdata-test-firewall --description=Access\ to\ Cloud\ Shell --direction=INGRESS --priority=1000 --network=default --action=ALLOW --rules=tcp:8088,tcp:50070,tcp:8080 --source-ranges=100.35.187.96/32


# Create a dataproc cluster
gcloud dataproc clusters create test-cluster --zone us-east4-a --master-machine-type n1-standard-1 --master-boot-disk-size 50 --num-workers 2 --worker-machine-type n1-standard-1 --worker-boot-disk-size 50 --network=default

# Delete a cluster
gcloud dataproc clusters delete test-cluster

# Create a bucket with the same name as project using gsutils

gsutil mb -c regional -l us-east4 gs://$DEVSHELL_PROJECT_ID

# Get the Google sample code from github on Compute instance
git clone https://github.com/GoogleCloudPlatform/training-data-analyst

#Move to folder 
cd training-data-analyst/courses/unstructured/

#Run the below script to copy files from compute local to google bucket
./replace_and_upload.sh rashnilsandeep


# enable gcloud commands on shell machine, scopes enables gcloud on machine
gcloud dataproc clusters create ${CLUSTERNAME} --scopes=cloud-platform --tags codelab --zone=us-east4-a

#Submit a Spark Job on cluster using gcloud
CLUSTERNAME=test-cluster
gcloud dataproc jobs submit spark  --cluster ${CLUSTERNAME} --class org.apache.spark.examples.SparkPi --jars file:///usr/lib/spark/examples/jars/spark-examples.jar -- 1000

#List of jobs running on cluster.
gcloud dataproc jobs list --cluster ${CLUSTERNAME}

# Adding cheap preemptible workers to the cluster
gcloud dataproc clusters update ${CLUSTERNAME} --num-preemptible-workers=2


#SSH into master node of the cluster
gcloud compute ssh ${CLUSTERNAME}-m -zone=us-east4-a

#Check firewall
gcloud compute firewall-rules list





